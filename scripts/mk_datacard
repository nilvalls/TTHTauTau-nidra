#!/usr/bin/env python

"""
===================
Creating a Datacard
===================

Usage
=====

Workflow / Customization
========================

Notes
=====
* Sample and discriminant names may not contain underscores (with the
  exception of the sample "data_obs")
* Indentation shall be four spaces
* "systematics.csv" shall be aligned at the delimiter "," with spaces.
  (best achieved with the `tabularized.vim` plugin)

ToDo
----
* doNorm
* summary of signal + background numbers for each category (stdout)

To Test
-------

Recently Tested
---------------
* b-tag uncertainty splitting
  * Inject b-tag uncertainties properly
  * `splitUncertainties_byCat.C`
* `copyQ2.C`
* `statUncertainties.C`
"""

import csv
import math
import os
import re
import ROOT as r
import sys

# The holy first line of PyROOT
r.gROOT.SetBatch()

# Define some regular expressions to match samples and signal.  The last
# part of the former expression is a negative look-ahead making sure that
# systematics are not caught by the category definition.
sample_re = re.compile(r'([^_]+(?:_obs)?)_([^_]+)_(.*)(?!(?:Up|Down)$)')
signal_re = re.compile(r'(ttH).*')

def get_ann_systematics(file, discriminant, categories, samples, data_sample="data_obs",
        signal_sample="ttH"):
    """For all samples and categories, this functions produces single bin
    systematics, shifts present in only one bin.  These shift will be
    produced when a process has significant expectation and the systematic
    is expected to contribute to the error.
    """
    new_sys = []

    for c in categories:
        data_hist = file.Get("{s}_{d}_{c}".format(s=data_sample, d=discriminant, c=c))
        sig_hist = file.Get("{s}_{d}_{c}".format(s=signal_sample + "125", d=discriminant, c=c))
        bkg_hist = None

        # Build background sum
        for (s, cats) in samples.items():
            s = s + "125" if s == "ttH" else s
            if s in (data_sample, signal_sample) or c not in cats:
                continue

            hist = file.Get("{s}_{d}_{c}".format(s=s, d=discriminant, c=c))
            if bkg_hist:
                bkg_hist.Add(hist)
            else:
                bkg_hist = hist.Clone()

        # Loop over samples for category and find low stats bins
        for (s, cats) in samples.items():
            file_s = s + "125" if s == "ttH" else s

            if c not in cats:
                continue

            hist = file.Get("{s}_{d}_{c}".format(s=file_s, d=discriminant, c=c))

            for b in range(1, hist.GetNbinsX() + 1):
                data = data_hist.GetBinContent(b)
                data_err = data_hist.GetBinError(b)

                sig = sig_hist.GetBinContent(b)
                sig_err = sig_hist.GetBinError(b)

                bkg = bkg_hist.GetBinContent(b)
                bkg_err = bkg_hist.GetBinError(b)

                val = hist.GetBinContent(b)
                val_err = hist.GetBinError(b)

                other_frac = math.sqrt(bkg_err**2 - val_err**2)

                if val < .01 or bkg_err < data_err / 3. or other_frac / bkg_err > .95 \
                        or sig / bkg < .02:
                    continue

                # FIXME Subtract 1 from bin name for comparability with
                # original C macro
                sys_name = "{s}_{c}_{e}_ANNbin{b:d}".format(
                        s=s, c=c, e="8TeV" if is_8_tev else "7TeV", b=b - 1)

                stub = "{s}_{d}_{c}_".format(s=file_s, d=discriminant, c=c)
                hist_up = hist.Clone(stub + sys_name + "Up")
                hist_up.SetBinContent(b, val + val_err)
                hist_down = hist.Clone(stub + sys_name + "Down")
                hist_down.SetBinContent(b, val - val_err)

                file.WriteObject(hist_up, hist_up.GetName())
                file.WriteObject(hist_down, hist_down.GetName())

                new_sys.append((sys_name, "shape",
                    dict([(sam, ("1" if sam == s else "-")) for sam in samples.keys()])))
    return new_sys

def get_integral(file, discriminant, category, sample="data_obs", uncertainty="", fmt="{n:.3f}", throw=False):
    """Get the integral of the histogram specified by the arguments.  The
    first argument has to be an open ROOT TFile.
    """
    if len(uncertainty) > 0:
        uncertainty = "_" + uncertainty
    h = file.Get("{s}_{d}_{c}{u}".format(s=sample, d=discriminant, c=category, u=uncertainty))
    i = h.Integral()
    if i == 0. and throw:
        raise Exception("The integral for {s}, {d}, {c}, {u} in {f} is zero!".format(
            s=sample, d=discriminant, c=category, u=uncertainty, f=file.GetName()))
    return fmt.format(n=h.Integral())

def get_samples(file, discriminant):
    """Reads the contents of `file`, an open ROOT TFile, and tries to
    extract available categories per sample.
    """
    samples = {}

    for k in f.GetListOfKeys():
        m = sample_re.match(k.GetName())
        if m:
            sample, disc, cat = m.groups()

            if disc != discriminant:
                continue

            m = signal_re.match(sample)
            if m:
                sample = m.group(1)

            if sample not in samples:
                samples[sample] = set()
            samples[sample].add(cat)
    return samples

def get_systematics(file, overrides={}, samples=False):
    """Reads `file` and returns a list of (uncertainty, type, {sample:
    value}).

    The parameter `overrides` allows to specify a dict of form
    {uncertainty: value}, and values of "x" n the systematics file are
    replaced by the one specified in the dict.

    The parameter `samples` can be set to `True` to obtain the available
    sample names defined in the systematics file.
    """
    reader = csv.DictReader(open(sysfile))
    reader.fieldnames = map(str.strip, reader.fieldnames)
    sys_samples = reader.fieldnames[2:]

    if samples:
        return sys_samples

    sys = []
    for row in reader:
        unc = row.pop("Uncertainty").strip()
        type = row.pop("Type").strip()
        row = dict(map(lambda (k,v): (k, v.strip()), row.items()))
        if unc in overrides:
            row = dict(
                    map(
                        lambda (k,v): (k, overrides[unc] if v == "x" else v),
                        row.items()))
        sys.append((unc, type, row))
    return sys

def split_q2(file, disc, which):
    """Split Q2-systematics by parton number as specified in `which`.
    """
    for (p, cats) in which.items():
        for c in cats:
            for d in ("Up", "Down"):
                try:
                    oldname = "ttbar_{d}_{c}_Q2scale_ttH_ttbar{dir}".format(d=disc, c=c, dir=d)
                    newname = "ttbar_{d}_{c}_Q2scale_ttH_ttbar{p}{dir}".format(d=disc, c=c, p=p, dir=d)
                    hold = file.Get(oldname)
                    hnew = hold.Clone(newname)
                    file.WriteObject(hnew, newname)
                except:
                    sys.stderr.write("Can't create Q^2 scale shifts for '{c}'\n".format(c=c))

def split_systematics(file, disc, samples):
    """Split b-tag uncertainties:  copy category histogram w/o systematics
    for rates, systematics of form "CMS_eff_bUp" to a shape uncertainty.

    The parameter `samples` should be a dictionary containing the
    categories per sample.

    Returns a list of systematics to be injected into the systematics file.
    """
    done = set()
    new_sys = []
    r.TH1.SetDefaultSumw2()

    for (s, cats) in samples.items():
        s = s + "125" if s == "ttH" else s
        for c in cats:
            stub = "_".join((s, disc, c))
            orig = file.Get(stub)
            sum = orig.Integral()
            for kind in ("eff", "fake"):
                for dir in ("Up", "Down"):
                    rate = orig.Clone("{s}_CMS_{k}_bRate{dir}".format(s=stub, k=kind, dir=dir))

                    try:
                        shape_old = file.Get("{s}_CMS_{k}_b{dir}".format(s=stub, k=kind, dir=dir))
                        shape = shape_old.Clone("{s}_{c}_{k}_bShape{dir}".format(s=stub, c=c, k=kind, dir=dir))

                        shape_sum = shape.Integral()
                        rate.Scale((shape_sum / sum) if sum > 0 else 1)
                        shape.Scale((sum / shape_sum) if shape_sum > 0 else 1)

                        file.WriteObject(rate, rate.GetName())
                        file.WriteObject(shape, shape.GetName())
                    except:
                        sys.stderr.write("Can't create b-tag shape uncertainties for '{s}'"
                                "in '{c}'\n".format(s=c, c=c))
                if c not in done:
                    new_sys.append((
                        "{c}_{k}_bShape".format(c=c, k=kind),
                        "shape",
                        dict([(sam, "1") for sam in samples.keys()])))
            done.add(c)
    return new_sys

# =============
# begin Program
# =============
execfile = sys.argv[0]
infile = sys.argv[1]
sysfile = os.path.join(os.path.dirname(execfile), "systematics.csv")

is_8_tev = True

# This replaces "x" in the systematics csv file with the values specified
# for certain uncertainties
overrides = {
        "lumi": "1.044" if is_8_tev else "1.022",
        "CMS_ttH_eff_lep": "1.04" if is_8_tev else "1.018",
        "CMS_ttH_QCDscale_ttbb": "1.5"}

# This defines which categories should be split into which parton number
parton_split = {
        "0p": "ljets_j4_t3 ljets_j4_t4 e2je2t SS_e3je1t SS_e3jge2t".split(),
        "1p": "ljets_j5_t3 ljets_j5_tge4 ge3t e3je2t SS_ge4je1t SS_ge4jge2t".split(),
        "2p": "ljets_jge6_t2 ljets_jge6_t3 ljets_jge6_tge4 ge4je2t".split()}

def jet_multiplicity(cat):
    if "e2je2t" in cat:
        return 2
    elif "3j" in cat or "ge3t" in cat:
        return 3
    elif "ge4j" in cat or "j4_t" in cat:
        return 4
    elif "j5_t" in cat:
        return 5
    elif "jge6_t" in cat:
        return 6
    return -1

f = r.TFile(infile, "UPDATE")

disc = "CFMlpANN"
categories = "ljets_j4_t3 ljets_j4_t4 ljets_j5_t3 ljets_j5_tge4 ljets_jge6_t2 ljets_jge6_t3 ljets_jge6_tge4 ge3t e3je2t ge4je2t SS_ge4je1t SS_e3jge2t SS_ge4jge2t TTL_j2_t1 TTL_j2_t2 TTL_j3_t1 TTL_jge4_t1 TTL_j3_t2 TTL_jge4_t2".split()

# Retrieve list of samples (ordered) from systematics file
samples = get_systematics(sysfile, samples=True)

# Get available categories for every sample
samples_cat = get_samples(f, disc)

# Trim previous to the samples defined in the systematics file
samples_cat = dict(filter(lambda (k,v): k in samples, samples_cat.items()))

# Filter categories to the ones defined above
cats = dict(map(
        lambda (k, v): (k, filter(lambda c: c in v, categories)),
        samples_cat.items()))

# Enumerate samples for combine
nums = dict([(s, n) for (n, s) in enumerate(samples)])

systematics = get_systematics(sysfile, overrides=overrides)
systematics += split_systematics(f, disc, cats)
systematics += get_ann_systematics(f, disc, categories, cats)

new_cats = set()
for (s, cs) in cats.items():
    for c in cs:
        new_cats.add(c)
categories = list(new_cats)

samples = filter(lambda s: s in cats, samples)

# import pprint
# print pprint.pprint(cats)
# print "FOOBAR"
# print pprint.pprint(systematics)
split_q2(f, disc, parton_split)

# Print preamble
print """imax * # number of channels
jmax * # number of backgrounds
kmax * # number of nuisance parameters
---------------"""

observed = map(lambda c: get_integral(f, disc, c, fmt="{n:.6f}"), categories)
print "bin " + " ".join(categories)
print "observation " + " ".join(map(str, observed))

if "data_obs" in cats:
    del cats["data_obs"]

print """---------------
shapes * * {f} $PROCESS_{d}_$CHANNEL $PROCESS_{d}_$CHANNEL_$SYSTEMATIC
shapes ttH * {f} $PROCESS$MASS_{d}_$CHANNEL $PROCESS$MASS_{d}_$CHANNEL_$SYSTEMATIC
---------------""".format(d=disc, f=infile)

sys.stdout.write("bin")
for s in samples:
    for c in cats[s]:
        sys.stdout.write(" " + c)
sys.stdout.write("\n")

# print "bin " + " ".join(categories * len(samples))
print "process" + "".join(map(lambda s: (" " + s) * len(cats[s]), samples))
print "process" + "".join(map(lambda s: (" " + str(nums[s])) * len(cats[s]), samples))
sys.stdout.write("rate" + " -1" * len(cats["ttH"]))
for s in samples[1:]:
    for c in cats[s]:
        sys.stdout.write(" " + str(get_integral(f, disc, c, s, fmt="{n:.6}")))
sys.stdout.write("\n")
print "---------------"

for (unc, type, vals) in systematics:
    if unc == "lumi":
        unc += "_8TeV" if is_8_tev else "_7TeV"

    for s in samples:
        file_s = s + "125" if s == "ttH" else s
        for c in cats[s]:
            if type == "shape" and vals[s] != "-":
                try:
                    get_integral(f, disc, c, file_s, unc + "Up", throw=True)
                    get_integral(f, disc, c, file_s, unc + "Down", throw=True)
                    sys.stdout.write(" " + vals[s])
                except:
                    sys.stderr.write("Integral not available for {s}, {c}, {u}: disabling "
                            "systematics\n".format(s=s, c=c, u=unc))
                    sys.stdout.write(" -")
            elif vals[s] in ["-", "1"] and not \
                    (unc == "Q2scale_ttH_V" and s in ("wjets", "zjets")):
                sys.stdout.write(" " + vals[s])
            else:
                if unc == "Q2scale_ttH_V" and s in ("wjets", "zjets"):
                    mult = jet_multiplicity(c)
                    if mult > 0:
                        vals[s] = str(1 + .1 * mult)
                    else:
                        sys.stdout.write(" -")
                        continue

                new_val = math.e ** (math.sqrt(
                    math.log(1 + (float(vals[s]) - 1)**2)))
                sys.stdout.write(" {n:.3f}".format(n=new_val))
    sys.stdout.write("\n")

print "---------------"
